HCP_SRC := $(TOP)/hcp
HCP_OUT ?= $(TOP)/build/hcp
MDIRS += $(HCP_OUT) $(TOP)/build

# If the default value for HCP_OUT gets used, it needs this dep
$(TOP)/build/hcp: | $(TOP)/build

# Used in dependency chains, as a change in these files can have effects that
# require rebuilding other things.
ifndef SAFEBOOT_HCP_RELAX
HCP_DEPS_COMMON := $(TOP)/Makefile $(HCP_SRC)/settings.mk $(HCP_SRC)/Makefile
endif

# Configuration settings. (From wherever. A developer, a CI pipeline, some
# package-building machinery, ...)
include hcp/settings.mk

# Utility script to simplify the workflow (depends on hcp/settings.mk)
ifeq (,$(SAFEBOOT_HCP_SUBMODULE_RESET_DISABLE))
ASSIST_CLEANUP=$(HCP_SRC)/assist_cleanup.sh
else
ASSIST_CLEANUP=DISABLE_SUBMODULE_RESET=1 $(HCP_SRC)/assist_cleanup.sh
endif

# Utility script to allow synchronization based on logfile/stdout/stderr
TAIL_WAIT=$(HCP_SRC)/tail_wait.pl

# To generate rules in smarter ways, first capture lists of images and networks
# that already exist.
HCP_EXISTING_IMAGES := $(shell docker image ls \
			--filter label=$(SAFEBOOT_HCP_DSPACE)all \
			--format "{{.Repository}}:{{.Tag}}" 2> /dev/null)
HCP_EXISTING_NETWORKS := $(shell docker network ls \
			--filter label=$(SAFEBOOT_HCP_DSPACE)all \
			--format "{{.Name}}" 2> /dev/null)

# There are various components that get produced as tarballs after being staged in
# host directories (under build/), and these are later consumed by Dockerfiles that
# have autogenerated commands appended to them by the build system. To avoid
# host oddities influencing the images (e.g. via umasks, setuid/setgid bits,
# ...), we use the following constructs for all tarball extractions;

# tarball_extract adds Dockerfile commands to copy foobar.tar.gz to the
# container image, extract it into a well-known temporary location, and then
# remove the tarball;
#     echo "$(call tarball_extract,foobar)" >> some/random/Dockerfile
tarball_extract = \# From tarball_extract($1)
tarball_extract += \nCOPY $1.tar.gz /
tarball_extract += \nRUN mkdir -p /tmp-hcp-tarball
tarball_extract += \nRUN cd /tmp-hcp-tarball && tar zxf /$1.tar.gz && rm /$1.tar.gz

# tarball_finalize adds Dockerfile commands to chown and chmod the directories
# and files from the well-known temporary directory, then move them all to the
# root directory and remove the temporary directory. Multiple tarball_extract
# sections can preceed the single (parameter-less) tarball_finalize section;
#     echo "$(call tarball_finalize)" >> some/random/Dockerfile
tarball_finalize = \# From tarball_finalize()
tarball_finalize += \nRUN cd /tmp-hcp-tarball && chown -R root:root *
tarball_finalize += \nRUN cd /tmp-hcp-tarball && find . -type d -exec chmod 755 {} \;
tarball_finalize += \nRUN cd /tmp-hcp-tarball && find . ! -type d -executable -exec chmod 755 {} \;
tarball_finalize += \nRUN cd /tmp-hcp-tarball && find . ! -type d ! -executable -exec chmod 644 {} \;
tarball_finalize += \nRUN cd /tmp-hcp-tarball && mv * / && cd / && rmdir /tmp-hcp-tarball

# A "base platform" Docker image is created, and used as a basis for deriving
# two kinds of images;
# - a "builder" container image, containing tool-chains and other extras to
#   facilitate the building of safeboot submodules.
# - application container images, providing the different services, side-cars,
#   and utilities that constitue HCP.
include hcp/base/Makefile

# A "builder" docker image is created that can host safeboot's building of
# submodule code (rather than building directly in the host environment, which
# can be undesirable, unreliable, or impossible for many reasons).
include hcp/builder/Makefile

# Compile submodules (using "builder"), resulting in install.tar.gz, literally
# a tarball of a root-level installation ("/install") of the submodules.
include hcp/submodules/Makefile

# Produce tarballs of scripts, for installation into containers. This includes
# safeboot.tar.gz (literally a tarball of safeboot scripts in an installed
# path) and hcp-<x>.tar.gz for <x> in {enrollsvc,attestsvc,swtpmsvc,client}.
include hcp/scripts/Makefile

# TODO: the creating, destroy, starting, and stopping of instances (both
# Enrollment and Attestation) is currently mingled into the Makefiles that
# build the service containers. This will change. But for now, we want those
# instances to connect to each other on a docker network, so provide the
# definitions and support before including the Enrollment/Attestation Service
# Makefiles.
include hcp/testnetwork.Makefile

# Enrollment Service
include hcp/enrollsvc/Makefile

# Attestation Service
include hcp/attestsvc/Makefile

# Software TPM
include hcp/swtpmsvc/Makefile

# Example client/host (containerized)
include hcp/client/Makefile

# A devel/debug image that has the whole kit and caboodle
include hcp/caboodle/Makefile

# Rule to build everything (unmentioned stuff, like submodules, is dragged in
# by dependency)
hcp_buildall: hcp_enrollsvc hcp_attestsvc hcp_swtpmsvc hcp_client hcp_caboodle

# The following (hcp/run) infrastructure/workflow is for automating the
# initialization, starting, stopping, and testing of HCP tools. It is
# engineered to be usable directly and independently, if the container images
# are already built/available and you want to have nothing to do with source
# code, build dependencies, etc. E.g. if a CI pipeline has already run the
# build and the resulting container images have been pushed to a production
# host, there is no source code and build tree on that prod host, so this
# infrastructure provides rules for running what is there, for direct use by
# admins and operators and/or via ansible or other tooling.
#
# But _here_, we integrate that workflow with the rest of our development
# workflow in order to provide a good developer. So how does that work?
#
# - We set the HCP_RUN_*** input variables that hcp/run/Makefile uses to
#   identify our images and paths.
# - We set HCP_RUN_SETTINGS to hcp/run/direct.mk, so that hcp/run/Makefile will
#   include it. These provide the application parameters! (If you're wondering
#   where things like port numbers, timeouts, hostnames, [...] get set, go look
#   there.)
# - We include hcp/run/Makefile to digest the above and produce rules for
#   operating the known services as well as output HCP_RUN_*** properties.
# - We use the output HCP_RUN_*** properties to create dependencies between the
#   service targets and (a) each other, and (b) the relevant build targets.
# - We bind cleanup dependencies to the rules/dependencies generated by
#   hcp/run/Makefile.
clean_hcp: clean_hcp_run
HCP_RUN_ASSIST_CLEANUP := $(ASSIST_CLEANUP)
HCP_RUN_DSPACE := $(SAFEBOOT_HCP_DSPACE)
HCP_RUN_DTAG := $(SAFEBOOT_HCP_DTAG)
HCP_RUN_DNETWORKS := $(HCP_TESTNETWORK_NAME)
HCP_RUN_UTIL_IMAGE := $(SAFEBOOT_HCP_BASE)
HCP_RUN_SETTINGS := $(HCP_SRC)/run/direct.mk

include $(HCP_SRC)/run/Makefile

# Build dependencies for hcp/run targets
ifdef NO_REINIT
HCP_REBUILD_DEP := |
endif
$(HCP_RUN_enrollsvc_TGT_INIT): $(HCP_REBUILD_DEP) $(HCP_ENROLLSVC_OUT)/built
$(HCP_RUN_attestsvc_TGT_INIT): $(HCP_REBUILD_DEP) $(HCP_ATTESTSVC_OUT)/built
$(HCP_RUN_swtpmsvc_TGT_INIT): $(HCP_REBUILD_DEP) $(HCP_SWTPMSVC_OUT)/built
$(HCP_RUN_client_TGT_START): $(HCP_REBUILD_DEP) $(HCP_CLIENT_OUT)/built
$(HCP_RUN_caboodle_TGT_START): $(HCP_REBUILD_DEP) $(HCP_CABOODLE_OUT)/built
# Docker network dependencies for hcp/run targets
$(HCP_RUN_enrollsvc_TGT_INIT) $(HCP_RUN_enrollsvc_mgmt_TGT_START) $(HCP_RUN_enrollsvc_repl_TGT_START) \
$(HCP_RUN_attestsvc_TGT_INIT) $(HCP_RUN_attestsvc_mgmt_TGT_START) $(HCP_RUN_attestsvc_repl_TGT_START) \
$(HCP_RUN_swtpmsvc_TGT_INIT) $(HCP_RUN_swtpmsvc_mgmt_TGT_START) $(HCP_RUN_swtpmsvc_repl_TGT_START) \
$(HCP_RUN_client_TGT_START) $(HCP_RUN_caboodle_TGT_START): \
	| $(HCP_OUT)/testnetwork.created
# Cleanup dependencies for hcp/run targets
clean_hcp_enrollsvc: enrollsvc_clean
clean_hcp_attestsvc: attestsvc_clean
clean_hcp_swtpmsvc: swtpmsvc_clean
# Inter-service dependencies;
$(HCP_RUN_attestsvc_TGT_INIT): | $(HCP_RUN_enrollsvc_repl_TGT_START)
$(HCP_RUN_swtpmsvc_TGT_INIT): | $(HCP_RUN_enrollsvc_mgmt_TGT_START)
$(HCP_RUN_client_TGT_START): | $(HCP_RUN_swtpmsvc_TGT_START)
$(HCP_RUN_client_TGT_START): | $(HCP_RUN_attestsvc_hcp_TGT_START)

# Rule to run everything
startall: enrollsvc_start attestsvc_start swtpmsvc_start client_start
stopall: enrollsvc_stop attestsvc_stop swtpmsvc_stop
runall: startall
	$Qecho "Services started and client completed, tearing down"
	$Q$(MAKE) stopall
clearall: enrollsvc_clean attestsvc_clean swtpmsvc_clean

# This target allows you to shell into a running container (using docker exec).
# If CID is set, it attempts to shell into the container it identifies,
# otherwise a listing of running containers is given.
enter:
ifeq (,$(CID))
	$Qecho "To shell into a running container, select the desired container by setting CID to"
	$Qecho "the corresponding 'CONTAINER ID'. Note, you only need to specify enough of the"
	$Qecho "initial hexadecimal characters to distinguish it from other containers."
	$Qecho
	$Qecho "Currently running containers in this project;"
	$Qdocker container ls --filter=label=$(SAFEBOOT_HCP_DSPACE)all
	$Qecho
	$Qecho "E.g.:   make enter CID=f00d"
	$Qecho
else
	$Qdocker exec -it $(CID) /bin/bash
endif

# This can be used as an order-only dependency (after a "|") for all "clean_*"
# rules that try to remove a container image. Why? Because even if we always
# pass "--rm" to docker-run, we can't entirely rid ourselves of
# exited-but-not-removed containers: if docker-build launches a container to
# run a Dockerfile command and it fails, _that_ container will linger, and in
# doing so it will prevent the removal of container images that are ancestors
# of it! Thus - this rule provides a way to detect that particular class of
# exited containers and remove them. Making sure it runs before your cleanup
# routine helps ensure your "docker imgae rm" statements don't fail.
preclean_hcp:
	$Qdocker container ls -a -q --filter=label=$(SAFEBOOT_HCP_DSPACE)all | \
		xargs -r docker container rm

clean_hcp:
	$Qrm -rf $(HCP_OUT)

# Make sure "clean_hcp" runs before "clean"
clean: clean_hcp
